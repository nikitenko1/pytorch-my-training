{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04374d4c",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 30px;\"> Long Short-Term Memory (LSTM) network with PyTorch</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7196c717",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 24px; color: magenta\"> Model A: 1 Hidden Layer</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acddf25",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 16px; color: magenta\"> Unroll 28 time steps</p>\n",
    "<p style=\"font-family:ComicSansMS; font-size: 22px; color: yellow\"> 1 Hidden layer</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5411a2",
   "metadata": {},
   "source": [
    "> Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "016cee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Dataset\n",
    "# Step 2: Make Dataset Iterable\n",
    "# Step 3: Create Model Class\n",
    "    # Step 4: Instantiate Model Class\n",
    "# Step 5: Instantiate Loss Class\n",
    "# Step 6: Instantiate Optimizer Class\n",
    "# Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8b8f84",
   "metadata": {},
   "source": [
    "> Step 1: Loading MNIST Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b38bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\38067\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:76: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "c:\\Users\\38067\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:66: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "c:\\Users\\38067\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:81: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "c:\\Users\\38067\\anaconda3\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:71: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "print(train_dataset.train_data.size())\n",
    "\n",
    "print(train_dataset.train_labels.size())\n",
    "\n",
    "print(test_dataset.test_data.size())\n",
    "\n",
    "\n",
    "print(test_dataset.test_labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400d1df",
   "metadata": {},
   "source": [
    "> Step 2: Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de9f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663bc59a",
   "metadata": {},
   "source": [
    "> Step 3: Create Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1582a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # 28 time steps\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea92d78",
   "metadata": {},
   "source": [
    "> Step 4: Instantiate Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09ef97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa151ae",
   "metadata": {},
   "source": [
    "> Step 5: Instantiate Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc83322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Long Short-Term Memory Neural Network: Cross Entropy Loss\n",
    "    # Recurrent Neural Network: Cross Entropy Loss\n",
    "    # Convolutional Neural Network: Cross Entropy Loss\n",
    "    # Feedforward Neural Network: Cross Entropy Loss\n",
    "    # Logistic Regression: Cross Entropy Loss\n",
    "    # Linear Regression: MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "866da5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb473ac",
   "metadata": {},
   "source": [
    "> Step 6: Instantiate Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d31f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even simplier equation\n",
    "# parameters = parameters - learning_rate * parameters_gradients\n",
    "# At every iteration, we update our model's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e51e3",
   "metadata": {},
   "source": [
    "> Mini-batch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fea4410",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14c5a9",
   "metadata": {},
   "source": [
    "> 1 Layer LSTM Groups of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a063734f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In-depth Parameters Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eed18c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400, 28])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32b52b",
   "metadata": {},
   "source": [
    "> Step 7: Train ModelÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "719ac66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process\n",
    "    # Convert inputs/labels to variables\n",
    "        # LSTM Input: (1, 28)\n",
    "        # RNN Input: (1, 28)\n",
    "        # CNN Input: (1, 28, 28)\n",
    "        # FNN Input: (1, 28*28)\n",
    "    # Clear gradient buffets\n",
    "    # Get output given inputs\n",
    "    # Get loss\n",
    "    # Get gradients w.r.t. parameters\n",
    "    # Update parameters using gradients\n",
    "    # parameters = parameters - learning_rate * parameters_gradients\n",
    "    # REPEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d0e88",
   "metadata": {},
   "source": [
    "> Training 1 Hidden Layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3c15639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 2.263676881790161. Accuracy: 19.610000610351562\n",
      "Iteration: 1000. Loss: 1.2170207500457764. Accuracy: 63.38999938964844\n",
      "Iteration: 1500. Loss: 0.8052400350570679. Accuracy: 80.2699966430664\n",
      "Iteration: 2000. Loss: 0.2070426493883133. Accuracy: 92.22000122070312\n",
      "Iteration: 2500. Loss: 0.19211973249912262. Accuracy: 95.16000366210938\n",
      "Iteration: 3000. Loss: 0.2757914066314697. Accuracy: 95.72000122070312\n"
     ]
    }
   ],
   "source": [
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as a torch tensor with gradient accumulation abilities\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Resize images\n",
    "                images = images.view(-1, seq_dim, input_dim)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d57fdf",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 24px; color: magenta\"> Model B: 2 Hidden Layer</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106601e5",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"font-family:ComicSansMS; font-size: 16px; color: magenta\"> Unroll 28 time steps</p>\n",
    "<p style=\"font-family:ComicSansMS; font-size: 22px; color: yellow\"> 2 Hidden layer</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff85d50f",
   "metadata": {},
   "source": [
    "> Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88dade3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Load Dataset\n",
    "# Step 2: Make Dataset Iterable\n",
    "# Step 3: Create Model Class\n",
    "    # Step 4: Instantiate Model Class\n",
    "# Step 5: Instantiate Loss Class\n",
    "# Step 6: Instantiate Optimizer Class\n",
    "# Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6de77",
   "metadata": {},
   "source": [
    "> Train 2 Hidden Layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21fe6629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(28, 100, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "10\n",
      "torch.Size([400, 28])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n",
      "Iteration: 500. Loss: 2.2840676307678223. Accuracy: 11.829999923706055\n",
      "Iteration: 1000. Loss: 2.05115008354187. Accuracy: 29.959999084472656\n",
      "Iteration: 1500. Loss: 0.9155963659286499. Accuracy: 67.20999908447266\n",
      "Iteration: 2000. Loss: 0.48642393946647644. Accuracy: 89.88999938964844\n",
      "Iteration: 2500. Loss: 0.21792744100093842. Accuracy: 94.22000122070312\n",
      "Iteration: 3000. Loss: 0.1300574541091919. Accuracy: 95.66999816894531\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # One time step\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 2  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# JUST PRINTING MODEL & PARAMETERS \n",
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as torch tensor with gradient accumulation abilities\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Resize image\n",
    "                images = images.view(-1, seq_dim, input_dim)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c99357e",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 24px; color: magenta\"> Model C: 3 Hidden Layer</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc709bb",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 16px; color: magenta\"> Unroll 28 time steps</p>\n",
    "<p style=\"font-family:ComicSansMS; font-size: 22px; color: yellow\"> 3 Hidden layer</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd7f65",
   "metadata": {},
   "source": [
    "> Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15540c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Dataset\n",
    "# Step 2: Make Dataset Iterable\n",
    "# Step 3: Create Model Class\n",
    "    # Step 4: Instantiate Model Class\n",
    "# Step 5: Instantiate Loss Class\n",
    "# Step 6: Instantiate Optimizer Class\n",
    "# Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d57eb",
   "metadata": {},
   "source": [
    "> 3 Hidden Layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1688e1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(28, 100, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "14\n",
      "torch.Size([400, 28])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400, 100])\n",
      "torch.Size([400])\n",
      "torch.Size([400])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n",
      "Iteration: 500. Loss: 2.2922871112823486. Accuracy: 11.350000381469727\n",
      "Iteration: 1000. Loss: 2.2976412773132324. Accuracy: 13.680000305175781\n",
      "Iteration: 1500. Loss: 2.04909348487854. Accuracy: 24.940000534057617\n",
      "Iteration: 2000. Loss: 0.7970125079154968. Accuracy: 66.11000061035156\n",
      "Iteration: 2500. Loss: 0.48532503843307495. Accuracy: 83.54000091552734\n",
      "Iteration: 3000. Loss: 0.32135140895843506. Accuracy: 91.05000305175781\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building your LSTM\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # One time step\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        # If we don't, we'll backprop all the way to the start even after going through another batch\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 3  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 10\n",
    "\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# JUST PRINTING MODEL & PARAMETERS \n",
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch Variable\n",
    "                images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9473f22",
   "metadata": {},
   "source": [
    "> Comparison with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91bca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A RNN\t    Model B RNN\t        Model C RNN\n",
    "# ReLU\t            ReLU\t            Tanh\n",
    "# 1 Hidden Layer\t2 Hidden Layers\t    3 Hidden Layers\n",
    "# 100 Hidden Units\t100 Hidden Units\t100 Hidden Units\n",
    "# 72.77%\t         64.70%\t            96.08%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09a04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A LSTM\t    Model B LSTM\t    Model C LSTM\n",
    "# 1 Hidden Layer\t2 Hidden Layers\t    3 Hidden Layers\n",
    "# 100 Hidden Units\t100 Hidden Units\t100 Hidden Units\n",
    "# 95.72%\t        95.66%\t            91.05%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
