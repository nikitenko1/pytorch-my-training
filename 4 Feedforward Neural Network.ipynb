{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4287f5d",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 30px;\"> Feedforward Neural Network with PyTorch</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc37a35",
   "metadata": {},
   "source": [
    "> Define logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "310e9037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "833a3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a7d11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "\n",
    "model = LogisticRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "046534f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel(\n",
      "  (linear): Linear(in_features=784, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577e27c",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 24px;\"> Building a Feedforward Neural Network with PyTorch</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5678cb",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 24px; color: magenta\"> Model A: 1 Hidden Layer Feedforward Neural Network (Sigmoid Activation)¶</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b0af0",
   "metadata": {},
   "source": [
    "> Steps¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b114ca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Dataset\n",
    "# Step 2: Make Dataset Iterable\n",
    "# Step 3: Create Model Class\n",
    "# Step 4: Instantiate Model Class\n",
    "# Step 5: Instantiate Loss Class\n",
    "# Step 6: Instantiate Optimizer Class\n",
    "# Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb48ed6",
   "metadata": {},
   "source": [
    "> Step 1: Loading MNIST Train Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd1c5b",
   "metadata": {},
   "source": [
    "> Images from 1 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c60ce3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3127be",
   "metadata": {},
   "source": [
    "> Step 2: Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffbef9c",
   "metadata": {},
   "source": [
    "> Batch sizes and iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71d0e6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Because we have 60000 training samples (images), we need to split them up to small groups (batches) and\n",
    "# pass these batches of samples to our feedforward neural network subsesquently.\n",
    "60000 / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d8cf1e",
   "metadata": {},
   "source": [
    "> Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03a3ba05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we want to go through the whole dataset 5 times (5 epochs) for the model to learn, \n",
    "# then we need 3000 iterations (600 x 5).\n",
    "600 * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d3654a",
   "metadata": {},
   "source": [
    "> Bringing batch size, iterations and epochs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfd613f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we have gone through above, we want to have 5 epochs, where each epoch would have 600 iterations \n",
    "# and each iteration has a batch size of 100.\n",
    "\n",
    "# Because we want 5 epochs, we need a total of 3000 iterations.\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f483db",
   "metadata": {},
   "source": [
    "> Step 3: Create Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4870fb2f",
   "metadata": {},
   "source": [
    "> Creating our feedforward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bbffab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to logistic regression with only a single linear layer, \n",
    "# we know for an FNN we need an additional linear layer and non-linear layer.\n",
    "\n",
    "# This translates to just 4 more lines of code!\n",
    "\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "\n",
    "        # Non-linearity\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function  # LINEAR\n",
    "        out = self.fc1(x)\n",
    "\n",
    "        # Non-linearity  # NON-LINEAR\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        # Linear function (readout)  # LINEAR\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95890038",
   "metadata": {},
   "source": [
    "> Step 4: Instantiate Model Class¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46363a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input dimension: 784\n",
    "# Size of image\n",
    "# Output dimension: 10\n",
    "# 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "# Hidden dimension: 100\n",
    "# Can be any number\n",
    "# Similar term\n",
    "# Number of neurons\n",
    "# Number of non-linear activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ca1b68",
   "metadata": {},
   "source": [
    "> Instantiating our model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0907f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca63ca",
   "metadata": {},
   "source": [
    "> Step 5: Instantiate Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0cd1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Feedforward Neural Network: Cross Entropy Loss\n",
    "# * Logistic Regression: Cross Entropy Loss\n",
    "# * Linear Regression: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a81925",
   "metadata": {},
   "source": [
    "> Loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43b3e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54717f4b",
   "metadata": {},
   "source": [
    "> Step 6: Instantiate Optimizer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89003d",
   "metadata": {},
   "source": [
    "Even simplier equation\n",
    "* parameters = parameters - learning_rate * parameters_gradients\n",
    "* **At every iteration, we update our model's parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb373a8",
   "metadata": {},
   "source": [
    "> Optimizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c415672",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f3596e",
   "metadata": {},
   "source": [
    "> Linear layers' parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a854258c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001EAE30B8AC0>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4fe23cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# This would return 4 because we've 2 linear layers, and each layer has 2 groups of parameters \n",
    "print(len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b24edce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784])\n"
     ]
    }
   ],
   "source": [
    "# Our first linear layer parameters, \n",
    "# , would be of size 100 x 784. This is because we've an input size of 784 (28 x 28) and a hidden size of 100.\n",
    "# FC 1 Parameters \n",
    "print(list(model.parameters())[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adab7e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Our first linear layer bias parameters, \n",
    "# , would be of size 100 which is our hidden size.\n",
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53abfd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "# Our second linear layer is our readout layer, where the parameters \n",
    "#  would be of size 10 x 100. This is because our output size is 10 and hidden size is 100.\n",
    "# FC 2 Parameters\n",
    "print(list(model.parameters())[2].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6eca7558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# FC 2 Bias Parameters\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64000881",
   "metadata": {},
   "source": [
    "> Step 7: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8e0bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process\n",
    "# Convert inputs to tensors with gradient accumulation capabilities\n",
    "# Clear gradient buffers\n",
    "# Get output given inputs\n",
    "# Get loss\n",
    "# Get gradients w.r.t. parameters\n",
    "# Update parameters using gradients\n",
    "# parameters = parameters - learning_rate * parameters_gradients\n",
    "# REPEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f2a1bf",
   "metadata": {},
   "source": [
    "> 7-step training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d62a0a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.5906487703323364. Accuracy: 85.87000274658203\n",
      "Iteration: 1000. Loss: 0.4183412492275238. Accuracy: 89.41000366210938\n",
      "Iteration: 1500. Loss: 0.2513048052787781. Accuracy: 90.45999908447266\n",
      "Iteration: 2000. Loss: 0.4173419177532196. Accuracy: 91.26000213623047\n",
      "Iteration: 2500. Loss: 0.42417335510253906. Accuracy: 91.8499984741211\n",
      "Iteration: 3000. Loss: 0.20444202423095703. Accuracy: 92.19000244140625\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507c017e",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 24px; color: magenta\"> Model B: 1 Hidden Layer Feedforward Neural Network (Tanh Activation)¶</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26f92398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps¶\n",
    "# Step 1: Load Dataset\n",
    "# Step 2: Make Dataset Iterable\n",
    "# Step 3: Create Model Class\n",
    "# Step 4: Instantiate Model Class\n",
    "# Step 5: Instantiate Loss Class\n",
    "# Step 6: Instantiate Optimizer Class\n",
    "# Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9993274",
   "metadata": {},
   "source": [
    "> 1-layer FNN with Tanh Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "06c5845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.484049528837204. Accuracy: 91.11000061035156\n",
      "Iteration: 1000. Loss: 0.27175629138946533. Accuracy: 92.30999755859375\n",
      "Iteration: 1500. Loss: 0.14635436236858368. Accuracy: 93.43000030517578\n",
      "Iteration: 2000. Loss: 0.16134881973266602. Accuracy: 94.12999725341797\n",
      "Iteration: 2500. Loss: 0.2215614914894104. Accuracy: 94.58000183105469\n",
      "Iteration: 3000. Loss: 0.20963969826698303. Accuracy: 94.97000122070312\n"
     ]
    }
   ],
   "source": [
    "# The only difference here compared to previously is that we are using Tanh activation \n",
    "# instead of Sigmoid activation. This affects step 3.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.tanh = nn.Tanh()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.tanh(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a5cf6",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 24px; color: magenta\"> Model C: 1 Hidden Layer Feedforward Neural Network (ReLU Activation)¶</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc7b444c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps¶\n",
    "# Step 1: Load Dataset\n",
    "# Step 2: Make Dataset Iterable\n",
    "# Step 3: Create Model Class\n",
    "# Step 4: Instantiate Model Class\n",
    "# Step 5: Instantiate Loss Class\n",
    "# Step 6: Instantiate Optimizer Class\n",
    "# Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904fca4",
   "metadata": {},
   "source": [
    "> 1-layer FNN with ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf6b997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.25051024556159973. Accuracy: 91.26000213623047\n",
      "Iteration: 1000. Loss: 0.22989000380039215. Accuracy: 92.87000274658203\n",
      "Iteration: 1500. Loss: 0.14008821547031403. Accuracy: 94.0\n",
      "Iteration: 2000. Loss: 0.2537596821784973. Accuracy: 94.72000122070312\n",
      "Iteration: 2500. Loss: 0.13707630336284637. Accuracy: 95.37000274658203\n",
      "Iteration: 3000. Loss: 0.10617245733737946. Accuracy: 95.73999786376953\n"
     ]
    }
   ],
   "source": [
    "# The only difference again is in using ReLU activation and it affects step 3.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        # Linear function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity\n",
    "        out = self.relu(out)\n",
    "        # Linear function (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce39ac91",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 24px; color: magenta\"> Model D: 2 Hidden Layer Feedforward Neural Network (ReLU Activation)¶</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2ed1e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps¶\n",
    "# Step 1: Load Dataset\n",
    "# Step 2: Make Dataset Iterable\n",
    "# Step 3: Create Model Class\n",
    "# Step 4: Instantiate Model Class\n",
    "# Step 5: Instantiate Loss Class\n",
    "# Step 6: Instantiate Optimizer Class\n",
    "# Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af6790",
   "metadata": {},
   "source": [
    "> 2-layer FNN with ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "84390aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.2887178063392639. Accuracy: 91.04000091552734\n",
      "Iteration: 1000. Loss: 0.2230677604675293. Accuracy: 93.58999633789062\n",
      "Iteration: 1500. Loss: 0.2839560806751251. Accuracy: 94.69999694824219\n",
      "Iteration: 2000. Loss: 0.08653215318918228. Accuracy: 95.9800033569336\n",
      "Iteration: 2500. Loss: 0.11454090476036072. Accuracy: 96.19999694824219\n",
      "Iteration: 3000. Loss: 0.07452377676963806. Accuracy: 96.69000244140625\n"
     ]
    }
   ],
   "source": [
    "# This is a bigger difference that increases your model's capacity\n",
    "#  by adding another linear layer and non-linear layer which affects step 3.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Linear function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Linear function 3 (readout): 100 --> 10\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Linear function 3 (readout)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "        labels = labels\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a748a8c",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 24px; color: magenta\"> Model E: 3 Hidden Layer Feedforward Neural Network (ReLU Activation)¶</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb2cc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps¶\n",
    "# Step 1: Load Dataset\n",
    "# Step 2: Make Dataset Iterable\n",
    "# Step 3: Create Model Class\n",
    "# Step 4: Instantiate Model Class\n",
    "# Step 5: Instantiate Loss Class\n",
    "# Step 6: Instantiate Optimizer Class\n",
    "# Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d90ff",
   "metadata": {},
   "source": [
    "> 3-layer FNN with ReLU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f64f9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.3862055540084839. Accuracy: 91.08000183105469\n",
      "Iteration: 1000. Loss: 0.1485331505537033. Accuracy: 94.1500015258789\n",
      "Iteration: 1500. Loss: 0.31284570693969727. Accuracy: 95.33000183105469\n",
      "Iteration: 2000. Loss: 0.1576518565416336. Accuracy: 95.2699966430664\n",
      "Iteration: 2500. Loss: 0.16087573766708374. Accuracy: 96.25\n",
      "Iteration: 3000. Loss: 0.07598868012428284. Accuracy: 96.44999694824219\n"
     ]
    }
   ],
   "source": [
    "# Let's add one more layer! Bigger model capacity. \n",
    "# But will it be better? Remember what we talked about on curse of dimensionality?\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "class FeedforwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(FeedforwardNeuralNetModel, self).__init__()\n",
    "        # Linear function 1: 784 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Linear function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Linear function 3: 100 --> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 3\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Linear function 4 (readout): 100 --> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "\n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu2(out)\n",
    "\n",
    "        # Linear function 2\n",
    "        out = self.fc3(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.relu3(out)\n",
    "\n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedforwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images with gradient accumulation capabilities\n",
    "        images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images with gradient accumulation capabilities\n",
    "                images = images.view(-1, 28*28).requires_grad_()\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
