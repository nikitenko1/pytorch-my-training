{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26dd99c7",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 30px;\"> Recurrent Neural Network with PyTorch</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51244ce",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 24px; color: magenta\"> Model A: 1 Hidden Layer (ReLU)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c9b700",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 16px; color: magenta\"> Unroll 28 time steps</p>\n",
    "<p style=\"font-family:ComicSansMS; font-size: 22px; color: yellow\"> 1 Hidden layer</p>\n",
    "<p style=\"font-family:ComicSansMS; font-size: 16px; color: magenta\"> ReLU Activation Function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55a531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unroll 28 time steps\n",
    "    # Each step input size: 28 x 1\n",
    "        # Total per unroll: 28 x 28\n",
    "        # Feedforward Neural Network input size: 28 x 28\n",
    "# 1 Hidden layer\n",
    "# ReLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97451390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps¶\n",
    "# Step 1: Load Dataset\n",
    "# Step 2: Make Dataset Iterable\n",
    "# Step 3: Create Model Class\n",
    "# Step 4: Instantiate Model Class\n",
    "# Step 5: Instantiate Loss Class\n",
    "# Step 6: Instantiate Optimizer Class\n",
    "# Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ff2113",
   "metadata": {},
   "source": [
    "> Step 1: Loading MNIST Train Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af3b481",
   "metadata": {},
   "source": [
    "> Looking into the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3557a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7f9ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:04<00:00, 2.06MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 223kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.31MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.27MB/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee6eb614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n",
      "torch.Size([10000, 28, 28])\n",
      "torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.train_data.size())\n",
    "\n",
    "print(train_dataset.train_labels.size())\n",
    "\n",
    "print(test_dataset.test_data.size())\n",
    "\n",
    "print(test_dataset.test_labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d91b9c",
   "metadata": {},
   "source": [
    "> Step 2: Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6144fbf",
   "metadata": {},
   "source": [
    "> Creating iterable objects to loop through subsequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c1c4ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2328ba23",
   "metadata": {},
   "source": [
    "> Step 3: Create Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53496b21",
   "metadata": {},
   "source": [
    "> 1 Layer RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5c64afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building your RNN\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, input_dim)\n",
    "        # batch_dim = number of samples per batch\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        # (layer_dim, batch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # This is part of truncated backpropagation through time (BPTT)\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 10\n",
    "        # out[:, -1, :] --> 100, 10 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c896b5f",
   "metadata": {},
   "source": [
    "> Step 4: Instantiate Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0297ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28 time steps\n",
    "    # Each time step: input dimension = 28\n",
    "# 1 hidden layer\n",
    "# MNIST 1-9 digits \n",
    "#  output dimension = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7219325",
   "metadata": {},
   "source": [
    "> Instantiate model class and assign to an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab1fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 1\n",
    "output_dim = 10\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f33d0af",
   "metadata": {},
   "source": [
    "> Step 5: Instantiate Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e601e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network: Cross Entropy Loss\n",
    "    # Convolutional Neural Network: Cross Entropy Loss\n",
    "    # Feedforward Neural Network: Cross Entropy Loss\n",
    "    # Logistic Regression: Cross Entropy Loss\n",
    "    # Linear Regression: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ceb74",
   "metadata": {},
   "source": [
    "> Cross Entropy Loss for Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e527e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aad0af",
   "metadata": {},
   "source": [
    "> Step 6: Instantiate Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fdb1784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even simplier equation\n",
    "    # parameters = parameters - learning_rate * parameters_gradients\n",
    "    # At every iteration, we update our model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1086e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4bac4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters In-Depth¶\n",
    "# Input to Hidden Layer Affine Function\n",
    "# A1, B1\n",
    "# Hidden Layer to Output Affine Function\n",
    "# A2, B2\n",
    "# Hidden Layer to Hidden Layer Affine Function\n",
    "# A3, B3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304315f5",
   "metadata": {},
   "source": [
    "> Total groups of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88d6d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7720ea92",
   "metadata": {},
   "source": [
    "> Input to Hidden Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8e554bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 28])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input --> Hidden (A1)\n",
    "list(model.parameters())[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53778d9e",
   "metadata": {},
   "source": [
    "> Input to Hidden Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e8a3917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Input --> Hidden BIAS (B1)\n",
    "list(model.parameters())[2].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c481a",
   "metadata": {},
   "source": [
    "> Hidden to Hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6b7aa73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 100])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden --> Hidden (A3)\n",
    "list(model.parameters())[1].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489b98de",
   "metadata": {},
   "source": [
    "> Hidden to Hidden Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19d13f39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden --> Hidden BIAS(B3)\n",
    "list(model.parameters())[3].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e4d741",
   "metadata": {},
   "source": [
    "> Hidden to Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "076148e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 100])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden --> Output (A2)\n",
    "list(model.parameters())[4].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4de75",
   "metadata": {},
   "source": [
    "> Hidden to Output Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf383920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden --> Output BIAS (B2)\n",
    "list(model.parameters())[5].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed36d3d5",
   "metadata": {},
   "source": [
    "> Step 7: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d01b4350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process\n",
    "    # Convert inputs/labels to tensors with gradient accumulation abilities\n",
    "        # RNN Input: (1, 28)\n",
    "        # CNN Input: (1, 28, 28)\n",
    "        # FNN Input: (1, 28*28)\n",
    "    # Clear gradient buffets\n",
    "    # Get output given inputs\n",
    "    # Get loss\n",
    "    # Get gradients w.r.t. parameters\n",
    "    # Update parameters using gradients\n",
    "    # parameters = parameters - learning_rate * parameters_gradients\n",
    "    # REPEAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508bc6b",
   "metadata": {},
   "source": [
    "> Same 7 step process for training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca924b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 2.300598382949829. Accuracy: 15.399999618530273\n",
      "Iteration: 1000. Loss: 2.3022639751434326. Accuracy: 16.3700008392334\n",
      "Iteration: 1500. Loss: 2.275730848312378. Accuracy: 20.09000015258789\n",
      "Iteration: 2000. Loss: 2.0255444049835205. Accuracy: 25.719999313354492\n",
      "Iteration: 2500. Loss: 1.3834927082061768. Accuracy: 61.16999816894531\n",
      "Iteration: 3000. Loss: 0.7578946948051453. Accuracy: 72.77999877929688\n"
     ]
    }
   ],
   "source": [
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            model.eval()\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to a Torch tensors with gradient accumulation abilities\n",
    "                images = images.view(-1, seq_dim, input_dim)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2836d4",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 24px; color: magenta\"> Model B: 2 Hidden Layer (ReLU)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683b7ad",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 16px; color: magenta\"> Unroll 28 time steps</p>\n",
    "<p style=\"font-family:ComicSansMS; font-size: 22px; color: yellow\"> 2 Hidden layer</p>\n",
    "<p style=\"font-family:ComicSansMS; font-size: 16px; color: magenta\"> ReLU Activation Function</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c3fcf",
   "metadata": {},
   "source": [
    "> Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb73854",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Step 1: Load Dataset\n",
    "    # Step 2: Make Dataset Iterable\n",
    "    # Step 3: Create Model Class\n",
    "        # Step 4: Instantiate Model Class\n",
    "    # Step 5: Instantiate Loss Class\n",
    "    # Step 6: Instantiate Optimizer Class\n",
    "    # Step 7: Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f39baeb",
   "metadata": {},
   "source": [
    "> 2 Hidden Layer + ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38450e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (rnn): RNN(28, 100, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "10\n",
      "torch.Size([100, 28])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n",
      "Iteration: 500. Loss: 2.2966396808624268. Accuracy: 10.279999732971191\n",
      "Iteration: 1000. Loss: 2.302166223526001. Accuracy: 11.479999542236328\n",
      "Iteration: 1500. Loss: 2.2946512699127197. Accuracy: 15.5600004196167\n",
      "Iteration: 2000. Loss: 2.2757158279418945. Accuracy: 18.469999313354492\n",
      "Iteration: 2500. Loss: 2.1378917694091797. Accuracy: 21.81999969482422\n",
      "Iteration: 3000. Loss: 1.1979039907455444. Accuracy: 64.70999908447266\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building your RNN\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # This is part of truncated backpropagation through time (BPTT)\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 2  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 10\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# JUST PRINTING MODEL & PARAMETERS \n",
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            model.eval()\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Resize images\n",
    "                images = images.view(-1, seq_dim, input_dim)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070874a5",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 24px; color: magenta\"> Model C: 2 Hidden Layer</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d88d0df",
   "metadata": {},
   "source": [
    "<p style=\"font-family:ComicSansMS; font-size: 16px; color: magenta\"> Unroll 28 time steps</p>\n",
    "<p style=\"font-family:ComicSansMS; font-size: 22px; color: magenta\"> 2 Hidden layer</p>\n",
    "<p style=\"font-family:ComicSansMS; font-size: 16px; color: yellow\"> Tanh Activation Function</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97b34a6",
   "metadata": {},
   "source": [
    "> Steps    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21488f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Dataset\n",
    "    # Step 2: Make Dataset Iterable\n",
    "    # Step 3: Create Model Class\n",
    "        # Step 4: Instantiate Model Class\n",
    "    # Step 5: Instantiate Loss Class\n",
    "    # Step 6: Instantiate Optimizer Class\n",
    "    # Step 7: Train Model\n",
    "# !!! \"2 Hidden + ReLU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7f65c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (rnn): RNN(28, 100, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n",
      "10\n",
      "torch.Size([100, 28])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n",
      "Iteration: 500. Loss: 0.9490087032318115. Accuracy: 72.1500015258789\n",
      "Iteration: 1000. Loss: 0.5472593903541565. Accuracy: 89.11000061035156\n",
      "Iteration: 1500. Loss: 0.44565892219543457. Accuracy: 81.16999816894531\n",
      "Iteration: 2000. Loss: 0.1718563288450241. Accuracy: 93.75\n",
      "Iteration: 2500. Loss: 0.23098306357860565. Accuracy: 95.87000274658203\n",
      "Iteration: 3000. Loss: 0.06793251633644104. Accuracy: 96.08999633789062\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "\n",
    "'''\n",
    "STEP 1: LOADING DATASET\n",
    "'''\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "'''\n",
    "STEP 2: MAKING DATASET ITERABLE\n",
    "'''\n",
    "\n",
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "'''\n",
    "STEP 3: CREATE MODEL CLASS\n",
    "'''\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(RNNModel, self).__init__()\n",
    "        # Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # Building your RNN\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='tanh')\n",
    "\n",
    "        # Readout layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n",
    "\n",
    "        # One time step\n",
    "        # We need to detach the hidden state to prevent exploding/vanishing gradients\n",
    "        # This is part of truncated backpropagation through time (BPTT)\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        # out.size() --> 100, 28, 100\n",
    "        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        # out.size() --> 100, 10\n",
    "        return out\n",
    "\n",
    "'''\n",
    "STEP 4: INSTANTIATE MODEL CLASS\n",
    "'''\n",
    "input_dim = 28\n",
    "hidden_dim = 100\n",
    "layer_dim = 2  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\n",
    "output_dim = 10\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# JUST PRINTING MODEL & PARAMETERS \n",
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "\n",
    "'''\n",
    "STEP 5: INSTANTIATE LOSS CLASS\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "'''\n",
    "learning_rate = 0.1\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "'''\n",
    "STEP 7: TRAIN THE MODEL\n",
    "'''\n",
    "\n",
    "# Number of steps to unroll\n",
    "seq_dim = 28  \n",
    "\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        # outputs.size() --> 100, 10\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        iter += 1\n",
    "\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Iterate through test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Resize images\n",
    "                images = images.view(-1, seq_dim, input_dim)\n",
    "\n",
    "                # Forward pass only to get logits/output\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "\n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Results \n",
    "# Model A\t            Model B\t            Model C\n",
    "# ReLU\t                ReLU\t            Tanh\n",
    "# 1 Hidden Layer\t    2 Hidden Layers\t    2 Hidden Layers\n",
    "# 100 Hidden Units\t    100 Hidden Units\t100 Hidden Units\n",
    "# 72.77%\t            64.70%\t            96.08%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0fc54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
